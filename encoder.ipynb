{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a0064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdteakhperky\u001b[0m (\u001b[33mdteakhperky-higher-school-of-economics\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "from torch import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import utils\n",
    "\n",
    "import monai\n",
    "import monai.transforms as mt\n",
    "\n",
    "import pypickle\n",
    "\n",
    "import medim\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dataset import get_dataset\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0944a35",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7913ddab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69372fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0xBAD5EED):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964726aa",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303031cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD_PATH = Path(os.getcwd())\n",
    "DATA_PATH = CWD_PATH / \"data\"\n",
    "\n",
    "ATLAS_DATA_PATH = DATA_PATH / \"Lumiere\" / \"atlas_mapping\"\n",
    "SPLIT_PATH = CWD_PATH / \"splits\" / \"split.pkl\"\n",
    "\n",
    "split = pypickle.load(SPLIT_PATH, verbose=\"silent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9525f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_image_keys = ['baseline_FLAIR', 'baseline_T1', 'baseline_T1CE', 'baseline_T2', 'baseline_seg']\n",
    "followup_image_keys = ['followup_FLAIR', 'followup_T1', 'followup_T1CE', 'followup_T2', 'followup_seg']\n",
    "keys = baseline_image_keys + followup_image_keys\n",
    "\n",
    "train_transform = mt.Compose([\n",
    "    mt.LoadImaged(keys=keys),\n",
    "    mt.EnsureChannelFirstd(keys=keys),\n",
    "    # mt.Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "    # Spatial augmentations\n",
    "    # mt.RandRotated(keys=keys, range_x=0.3, prob=0.5),\n",
    "    # mt.RandFlipd(keys=keys, prob=0.5),\n",
    "    # mt.RandZoomd(keys=keys, min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "    # Intensity augmentations\n",
    "    # mt.RandGaussianNoised(keys=keys, std=0.01, prob=0.2),\n",
    "    # mt.RandAdjustContrastd(keys=keys, gamma=(0.7, 1.3), prob=0.3),\n",
    "    # Normalization\n",
    "    mt.ScaleIntensityd(keys=keys),\n",
    "    mt.ConcatItemsd(keys=keys, name=\"images\"),\n",
    "    mt.DeleteItemsd(keys=keys),\n",
    "    mt.RandFlipd(keys=[\"images\"]),\n",
    "    mt.ResizeD(keys=[\"images\"], spatial_size=(121, 128, 121)),\n",
    "    # mt.RandCropByPosNegLabeld(\n",
    "    #     keys=[\"images\"],\n",
    "    #     label_key=\"baseline_seg\",\n",
    "    #     spatial_size=(128, 128, 64),\n",
    "    #     pos=1, neg=1, num_samples=4,\n",
    "    #     image_key=\"images\", prob=0.5\n",
    "    # ),\n",
    "    # mt.ConcatItemsd(keys=followup_image_keys, name=\"followup\"),\n",
    "    mt.ToTensord(keys=[\"images\"])\n",
    "])\n",
    "\n",
    "valid_transform = mt.Compose([\n",
    "    mt.LoadImaged(keys=keys),\n",
    "    mt.EnsureChannelFirstd(keys=keys),\n",
    "    # mt.Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "    # mt.ConcatItemsd(keys=baseline_image_keys, name=\"baseline\", dim=0),\n",
    "    # mt.ConcatItemsd(keys=followup_image_keys, name=\"followup\", dim=0),\n",
    "    mt.ConcatItemsD(keys=keys, name=\"images\", dim=0),\n",
    "    mt.DeleteItemsd(keys=keys),\n",
    "    mt.ResizeD(keys=[\"images\"], spatial_size=(121, 128, 121)),\n",
    "    # mt.ToTensord(keys=[\"baseline\", \"followup\"]),\n",
    "    mt.ToTensord(keys=[\"images\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922400d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 84)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = get_dataset(\n",
    "    data_path=ATLAS_DATA_PATH,\n",
    "    indices=split[\"train_patient_ids\"],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "valid_dataset = get_dataset(\n",
    "    data_path=ATLAS_DATA_PATH,\n",
    "    indices=split[\"valid_patient_ids\"],\n",
    "    transform=valid_transform\n",
    ")\n",
    "\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b857614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0][\"images\"].shape, valid_dataset[0][\"images\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afcf612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae7dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))[\"images\"].shape, next(iter(valid_loader))[\"images\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35cc21cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f9f6a",
   "metadata": {},
   "source": [
    "# Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "741299f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def test_model_correctness(model, loader):\n",
    "    images = next(iter(loader))[\"images\"]\n",
    "    images = images.to(device)\n",
    "    model = model.to(device)\n",
    "    outputs = model(images[:, :5], images[:, 5:]).cpu().numpy()\n",
    "    assert outputs.shape[-1] == 4, f'Wrong last output dimension. Expected {4}; Got {outputs.shape[-1]}'\n",
    "    print('All gucci')\n",
    "\n",
    "def calculate_receptive_field(params: list[tuple[int, int]]) -> int:\n",
    "    r, s = 1, 1\n",
    "    for kernel_size, stride in params:\n",
    "        r += (kernel_size - 1) * stride\n",
    "        s *= stride\n",
    "    \n",
    "    return r\n",
    "\n",
    "def calculate_params(model: torch.nn.Module) -> int:\n",
    "    return f'{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f} M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "321d5992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, \n",
    "    loader, \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler=None, \n",
    "    wandb_logging=False,\n",
    "    accumulation_steps=1\n",
    "):\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(tqdm(loader, leave=False)):\n",
    "        batch[\"images\"] = batch[\"images\"].to(device)\n",
    "        batch[\"label\"] = batch[\"label\"].to(device)\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "            outputs = model(batch[\"images\"][:, :5], batch[\"images\"][:, 5:])\n",
    "            loss = loss_fn(outputs, batch[\"label\"]) / accumulation_steps  # Scale loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        accuracy = np.mean(preds == batch[\"label\"].cpu().numpy())\n",
    "        total_acc += accuracy\n",
    "        \n",
    "        step_count += 1\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if wandb_logging:\n",
    "                wandb.log({\n",
    "                    'train_loss': total_loss / step_count,\n",
    "                    'train_accuracy': total_acc / step_count,\n",
    "                    'learning_rate': optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            step_count = 0\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return {\n",
    "        'train_loss': total_loss / max(1, step_count),\n",
    "        'train_accuracy': total_acc / max(1, step_count)\n",
    "    }\n",
    "        \n",
    "@torch.inference_mode()\n",
    "def evaluate(model, loader, loss_fn, device, scheduler=None, wandb_logging=False):\n",
    "    model.eval()\n",
    "    batches_cnt = 0\n",
    "    accuracy_acum = 0\n",
    "    accuracy_per_class = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "    samples_cnt_per_class = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "\n",
    "    for batch in tqdm(loader, leave=False):\n",
    "        batch[\"images\"] = batch[\"images\"].to(device)\n",
    "        batch[\"label\"] = batch[\"label\"].to(device)\n",
    "\n",
    "        outputs = model(batch[\"images\"][:, :5], batch[\"images\"][:, 5:])\n",
    "        loss = loss_fn(outputs, batch[\"label\"])\n",
    "\n",
    "        # ===================\n",
    "        # METRICS CALCULATION\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        accuracy = np.mean(preds == batch[\"label\"].cpu().numpy())\n",
    "        for i in range(NUM_CLASSES):\n",
    "            accuracy_per_class[i] += np.sum(preds[batch[\"label\"].cpu().numpy() == i] == i) if np.any(batch[\"label\"].cpu().numpy() == i) else 0\n",
    "            samples_cnt_per_class[i] += np.sum(batch[\"label\"].cpu().numpy() == i)\n",
    "        accuracy_acum += accuracy\n",
    "        batches_cnt += 1\n",
    "        # ===================\n",
    "    \n",
    "    valid_logs = {\n",
    "        'valid_loss': loss.item(),\n",
    "        'valid_accuracy': accuracy_acum / batches_cnt,\n",
    "        'valid_accuracy_0': accuracy_per_class[0] / samples_cnt_per_class[0],\n",
    "        'valid_accuracy_1': accuracy_per_class[1] / samples_cnt_per_class[1],\n",
    "        'valid_accuracy_2': accuracy_per_class[2] / samples_cnt_per_class[2],\n",
    "        'valid_accuracy_3': accuracy_per_class[3] / samples_cnt_per_class[3],\n",
    "    }\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "        \n",
    "    if wandb_logging:\n",
    "        wandb.log(valid_logs)\n",
    "    \n",
    "    return valid_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74a9ff",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04e50d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb26af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, emb_dim: int, dropout: float):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(2 * emb_dim, 4 * emb_dim),\n",
    "            nn.LayerNorm(4 * emb_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * emb_dim, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, baseline, followup):\n",
    "        baseline_embed = self.encoder(baseline)\n",
    "        followup_embed = self.encoder(followup)\n",
    "        # joint_embed = self.encoder(followup - baseline)\n",
    "        joint_embed = torch.cat([baseline_embed, followup_embed], dim=1)\n",
    "        outputs = self.head(joint_embed)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 5\n",
    "EMB_DIM = 128\n",
    "DROPOUT = 0.50\n",
    "\n",
    "def get_model() -> nn.Module:\n",
    "    encoder = monai.networks.nets.DenseNet169(\n",
    "        spatial_dims=2,\n",
    "        in_channels=5,\n",
    "        out_channels=EMB_DIM,\n",
    "        # att_dropout=0.1\n",
    "        pretrained=True,\n",
    "    )\n",
    "    baseline_model = BaselineModel(\n",
    "        encoder=encoder,\n",
    "        emb_dim=EMB_DIM,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    return baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e249aab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Parameter `spatial_dims` is > 2 ; currently PyTorch Hub does notprovide pretrained models for more than two spatial dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m baseline_model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m test_model_correctness(baseline_model, train_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m() -> nn.Module:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     encoder = \u001b[43mmonai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnetworks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDenseNet169\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEMB_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# att_dropout=0.1\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     baseline_model = BaselineModel(\n\u001b[32m     14\u001b[39m         encoder=encoder,\n\u001b[32m     15\u001b[39m         emb_dim=EMB_DIM,\n\u001b[32m     16\u001b[39m         dropout=DROPOUT\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m baseline_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/mri/lib/python3.11/site-packages/monai/networks/nets/densenet.py:355\u001b[39m, in \u001b[36mDenseNet169.__init__\u001b[39m\u001b[34m(self, spatial_dims, in_channels, out_channels, init_features, growth_rate, block_config, pretrained, progress, **kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spatial_dims > \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    356\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mParameter `spatial_dims` is > 2 ; currently PyTorch Hub does not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    357\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprovide pretrained models for more than two spatial dimensions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    358\u001b[39m         )\n\u001b[32m    359\u001b[39m     _load_state_dict(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdensenet169\u001b[39m\u001b[33m\"\u001b[39m, progress)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Parameter `spatial_dims` is > 2 ; currently PyTorch Hub does notprovide pretrained models for more than two spatial dimensions."
     ]
    }
   ],
   "source": [
    "baseline_model = get_model()\n",
    "test_model_correctness(baseline_model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "baf4b683",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'baseline_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m calculate_params(\u001b[43mbaseline_model\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'baseline_model' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_params(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89223bd9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a732043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(\n",
    "    epochs, model, train_loader, valid_loader, loss_fn, optimizer, device,\n",
    "    scheduler=None, wandb_logging=False,\n",
    "    checkpoint_freq: int = None, checkpoint_name: str = None\n",
    "):\n",
    "    seed_everything()\n",
    "    model = model.to(device)\n",
    "    train_logs, valid_logs = [], []\n",
    "\n",
    "    if wandb_logging:\n",
    "        wandb.init(\n",
    "            project=\"MRI Classification\",\n",
    "            name=\"FFA\",\n",
    "            config={\"desc\": \"Doing something\"}\n",
    "        )\n",
    "\n",
    "    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "        train_scheduler = None\n",
    "        valid_scheduler = scheduler\n",
    "    else:\n",
    "        train_scheduler = scheduler\n",
    "        valid_scheduler = None\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_log = train(\n",
    "            model=model,\n",
    "            loader=train_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            scheduler=train_scheduler,\n",
    "            wandb_logging=wandb_logging\n",
    "        )\n",
    "        train_logs.append(train_log)\n",
    "\n",
    "        valid_log = evaluate(\n",
    "            model=model,\n",
    "            loader=valid_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "            scheduler=valid_scheduler,\n",
    "            wandb_logging=wandb_logging\n",
    "        )\n",
    "        valid_logs.append(valid_log)\n",
    "\n",
    "        if checkpoint_freq is not None:\n",
    "            checkpoint = {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"train_scheduler_state\": None if train_scheduler is None else train_scheduler.state_dict(),\n",
    "                \"valid_scheduler_state\": None if valid_scheduler is None else valid_scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, PATH_TO_MODELS / f\"{checkpoint_name}_{epoch + 1}.pth\")\n",
    "            # REMOVE PREVIOUS CHECKPOINT\n",
    "            if epoch + 1 != checkpoint_freq:\n",
    "                os.remove(PATH_TO_MODELS / f\"{checkpoint_name}_{epoch + 1 - checkpoint_freq}.pth\")\n",
    "\n",
    "        if wandb_logging:\n",
    "            wandb.log({'learning_rate': optimizer.param_groups[0]['lr']})\n",
    "    \n",
    "    if wandb_logging:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return train_logs, valid_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16a9736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 25\n",
    "WEIGHT_DECAY = 1e-5\n",
    "MOMENTUM = 0.90\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "TEMP = 0.80\n",
    "CLASS_WEIGHTS = 1 / (torch.tensor([0.04, 0.08, 0.23, 0.65])) ** TEMP\n",
    "\n",
    "model = get_model()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS.to(device))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10a3a66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniil.tikhonov/mri/wandb/run-20250626_124909-tal7luxe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification/runs/tal7luxe' target=\"_blank\">FFA</a></strong> to <a href='https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification' target=\"_blank\">https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification/runs/tal7luxe' target=\"_blank\">https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification/runs/tal7luxe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [2:09:59<15:56, 318.68s/it]  IOStream.flush timed out\n",
      "100%|██████████| 25/25 [2:28:06<00:00, 355.45s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▅▄▁▅▄▄▅▅▆▄▅▄▃▃▅▃█▅█▄▅▄▄▃▂▅▄▆▇▅▇▅▄▆▆▅▆▄▆▅</td></tr><tr><td>train_loss</td><td>█▄▅▃▃▃▃▄▄▄▄▄▃▃▃▅▃▃▃▄▄▂▂▄▂▃▅▅▅▃▂▂▃▃▅▂▂▃▄▁</td></tr><tr><td>valid_accuracy</td><td>▅█▇███▆▇▇▄▇▇▆▆▅█▇█▆▄▆▆▆▄▁</td></tr><tr><td>valid_accuracy_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_accuracy_2</td><td>▅▁▂▁▁▂▅▂▃▆▄▁▆▁▅▂▄▄▅▄▄▄█▆█</td></tr><tr><td>valid_accuracy_3</td><td>▅█▇███▆▇▇▄▆▇▅▇▅█▇▇▆▅▆▆▄▄▁</td></tr><tr><td>valid_loss</td><td>▆▆▅▅▄▃▄▆▄▆▇▆▆█▅▅▅▅▁▄▄▂▅▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>train_accuracy</td><td>0.6875</td></tr><tr><td>train_loss</td><td>1.10683</td></tr><tr><td>valid_accuracy</td><td>0.44048</td></tr><tr><td>valid_accuracy_1</td><td>0</td></tr><tr><td>valid_accuracy_2</td><td>0.4375</td></tr><tr><td>valid_accuracy_3</td><td>0.55556</td></tr><tr><td>valid_loss</td><td>1.27537</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">FFA</strong> at: <a href='https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification/runs/tal7luxe' target=\"_blank\">https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification/runs/tal7luxe</a><br> View project at: <a href='https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification' target=\"_blank\">https://wandb.ai/dteakhperky-higher-school-of-economics/MRI%20Classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250626_124909-tal7luxe/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_logs, val_logs = run_training_pipeline(\n",
    "    epochs=NUM_EPOCHS,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    scheduler=scheduler,\n",
    "    wandb_logging=True,\n",
    "    checkpoint_freq=None,\n",
    "    checkpoint_name=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f1a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8e3ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
